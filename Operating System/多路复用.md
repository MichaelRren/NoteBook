# 多路复用
## 操作系统
GDT -> 开启保护模式，划分用户空间和内核空间。内核空间提供系统调用，让应用程序使用软中断进行调用。IO 是有成本的，app 使用 IO 一定会调用内核。

strace -ff -0 ./xx.txt java TestSocket: 抓取程序的系统调用

jps 查询正在运行的  java 进程

cd /proc/xxx 进入到进程 xxx 中，这个进程中的属性、线程也都是以文件的形式存储在当前目录中。在 task 目录中为线程/ 以及 fd 目录，数字代表文件描述符。任何一个都有 0 1 2 分别代表 输入、输出和错误。

最原始的 BIO 使用多线程解决多客户端连接。只有 ServerSocket 才能有监听的状态。程序调用 socket 的系统调用，然后获得了一个文件描述符。并且要绑定一个端口号，用文件描述符进行监听；当有连接时，就要使用 accpet() 去接收 文件描述符中是否有连接，阻塞。创建一个新的文件描述符，与之前的文件描述符相连。每 accept 一个文件描述符，就会 clone 一个新的线程。每一个线程对应一个 client 连接。

缺点是 连接太多，线程过多，线程的创建和销毁导致系统调用多。

由于系统内核的升级，提供了 select 系统调用，select 就叫多路复用器，提供的是返回的状态，程序自己发生读写的事件。但是依旧是同步的。内核需要 O(n) 主动遍历文件描述符的集合；并且内核空间和用户空间需要传递数据。

1. epoll_create 返回一个文件描述符，描述的是内核中的一个区域，拿到内存地址; 
2. epoll_ctl(epoll_create 的空间, add 操作，监听的文件描述符，accept)
3. epoll_wait(create 的文件描述符)
4. accept 创建连接的 socket
5. epoll_ctl(空间，创建的 socket)

redis 进行轮询。一个线程需要进行 LRU RDB AOF 等操作。

## 目的
设计一个高可用的网络服务器，在服务器中，网络连接是以文件描述符的形式存储的。对于单线程
```C++
while(1) {
    for (Fdx in (Fd)) {
      if (Fdx 有数据) {
        读取 Fdx;
        处理; 
      }
  }
}
```
### Select
```C
//准备文件描述符的数组 fds
sockfd = socket(AF_INET, SOCK_STREAM, 0);
memset(&addr, 0, sizeof(addr));
addr.sin_family = AF_INET;
addr.sin_port = htons(2000);
add.sin_addr.s_addr = INADDR_ANY;
bind(sockfd, (struct sockaddr*)&addr, sizeof(addr));
listen(sockfd, 5);
  
for(i = 0; i < 5; i++) {
    memset(&client, 0, sizeof(client));
    addrlent = sizeof(client);
    //socket 接受 5个 连接，随机的
    fds[i] = accept(sockfd, (struct sockaddr*)&client, &addrlen);
    if (fds[i] > max) max = fds[i];
}

while(1) {
    FD_ZERO(&rset);
    for (i = 0; i < 5; i++) {
        // &rset 是一个 bitmap,被占用的位置就标为 1，表示被监听。一共 1024个位置。
        FD_SET(fds[i], &rset);
    }
    
    puts("round agagin");
    //最大的文件描述符 + 1；读文件描述符，写文件描述符和异常描述符；超时时间
    select(max + 1, &rset, NULL, NULL, NULL);
    
    for(i = 0; i < 5; i++) {
        if (FD_ISSET(fds[i], &rset)) {
            memset(buffer, 0, MAXBUF);
            read(fds[i], buffer, MAXBUF);
            puts(buffer);
        }
    }
}
```
用户空间的程序运行时存在着一个 rset，用来标识文件描述符；select 函数在运行时会将用户空间的 rset 拷贝到 内核空间，每个 fd 中是否有数据。如果没有数据，内核态的这段 select 会阻塞。

如果有数据，
1. 会将有数据的 FD 置位(fdset中的 fd)，注意这里 FD 置位中 FD 指的其实是 rset 中对应的那一位，而不是真正的 fds 中的元素。
2. select 程序不再阻塞，会返回。接着判断哪个 fd 被置位了，读取被置位的 FD 并处理。

总结：select 函数 将文件描述符收集交给内核态，然后内核判断有数据，当有数据时会返回并置位，此时遍历fd集合，看哪个被置位，读取这个 fd 的数据并处理。提高处理的关键在于将判读的操作交由内核态进行处理

缺点：
1. bitmap 大小为 1024
2. fdset 不可重用，每次进入 while 时 都要重新置位成 0。
3. 拷贝过程的开销，用户态和内核态的开销
4. 内核态遍历也需要 O(n) 复杂度

### poll
```C
struct pollfd {
    //存入 fd
    int fd;
    // 关注的事件 读POLLIN 写POLLOUT 读写 POLLIN&POLLOUT 
    short events;
    // 对 event 的返回，初始为 0
    short revents;
};

for (i = 0; i < 5; i++) {
    memset(&client, 0, sizeof(client));
    addrlen = sizeof(client);
    //将 fd 直接保存到了 pollfds 中
    pollfds[i].fd = accept(sockfd, (struct sockaddr*)&client, &addrlen);
    //只在意读事件
    pollfds[i].events = POLLIN;
}
sleep(1);

while(1) {
    puts("round again");
    //pollfds 是 pollfd 结构体的数组，5个元素以及超时时间。
    //依旧是阻塞函数
    poll(pollfds, 5, 50000);
    
    for (i = 0; i < 5; i++) {
        if (pollfds[i].revents & POLLIN) {
            pollfds[i].revents = 0;
            memset(buffer, 0, MAXBUF);
            read(pollfds[i].fd, buffer, MAXBUF);
            puts(buffer);
        }
    }
}
```
用户态依旧有 5 个 fd，将用户态的 fd 拷贝到 内核态，依旧是内核态用于监听数据。优势在于没有使用 bitmap 而是使用了声明的结构体 pollfd；poll函数同样是阻塞的函数，同样会判断 pollfds 中是否有数据

如果有数据
1. 将 pollfds 中的 revents 字段进行置位。之前直接将 bitmap 进行置位，导致 bitmap 不可重用。一旦有读的事件，就会将 revents 置位 POLLIN
2. poll 返回，判断是否事件为 POLLIN，如果为 POLLIN，就先将这个位置的  revents 重置 为 0，接着进行处理。

1. pollfd 的数组大小远超过 1024
2. 结构体每次置位 revents，每次只需要复位 revents 字段就好，
3. 但是并没有解决遍历的复杂度以及切换的开销

### [Epoll](https://blog.csdn.net/armlinuxww/article/details/92803381)

```C
struct epoll_event events[5];
int epfd = epoll_create(10);
...
...

for (i = 0; i < 5; i++) {
    static struct epoll_event ev;
    memset(&client, 0, sizeof(client));
    addrlen = sizeof(client);
    ev.data.fd = accept(sockfd, (struct sockaddr*)&client, &addrlent);
    ev.events = EPOLLIN;
    epoll_ctl(epfd, EPOLL_CTL_ADD, ev.data.fd, &ev);
}
while(1) {
    puts("round again");
    nfds = epoll_wait(epfd, events, 5, 10000);
    
    for(i = 0; i < nfds; i++) {
        memset(buffer, 0, MAXBUF);
        read(events[i].data.fd, buffer, MAXBUF);
        puts(buffer);
    }
}
```
1. 使用 epoll_create 时，会创建epfd他是内核在 epoll 文件系统中创建了一个 file 节点， 在内核 cache 里建了个红黑树用于存储以后 epoll_ctl 传来的 socket，还会再建一个 list 链表，用于存储准备就绪的事件。
2. 使用epoll_ctl 对这个 epoll 进行配置。首先定一个事件，添加：加入一个文件描述符以及对应的 events 结构体(fd-events)。 这个结构体是 epoll_event 类型的，再通过 epoll_ctl 将需要监视的 Socket 添加到 Epfd 中，最后调用 epoll_wait 等待数据
3. 可以看到，经历完第一个循环后，此时的 epfd 中维护了 五个 ev 结构体元素，并且维护好内部的数据 fd 和 events
4. 执行死循环时，在 epoll_wait 中用户态和内核态共享 epfd
5. 有数据时，通过重排置位，将有数据的 fd 放在 epfd 中的最前面的位置。然后进行返回。带有返回值的返回，返回带有数据的 fd (nfds 的值)，此时就只需要处理 nfds 中的前几个元素。因此此时的复杂度为 O(1);

  